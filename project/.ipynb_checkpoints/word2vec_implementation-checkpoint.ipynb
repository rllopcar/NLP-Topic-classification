{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec implementation for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import errno\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "filepath = '../data/bbc/'\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the labels names and saving then in \"labels_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(filepath)\n",
    "for name in files:\n",
    "    if os.path.isdir(filepath+name): \n",
    "        labels.append(name)\n",
    "        \n",
    "text = []   \n",
    "texts_aux = [] \n",
    "texts_labels = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the content of each .txt document and label the content of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    path = filepath+label+'/*.txt'\n",
    "    files = glob.glob(path)\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name, 'r',encoding='ISO-8859-1') as f:\n",
    "                texts_aux.append(f.read())\n",
    "                texts_aux.append(label)\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise\n",
    "        texts_labels.append(texts_aux)\n",
    "        texts_aux=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled texts stored in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_labels_np = np.array(texts_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled texts stored in panda dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(texts_labels, columns=['text','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: Tokenize each text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "## Load library for removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "##nltk.download('stopwords') --> First time has to be downloaded\n",
    "\n",
    "# Import libraries for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "stemmer_ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem.cistem import Cistem\n",
    "stemmer_cs = Cistem()\n",
    "\n",
    "# Import lemmatization libraries\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#nltk.download('wordnet')--> First time has to be downloaded \n",
    "\n",
    "# Load stop words \n",
    "stop_words = stopwords.words('english')\n",
    "#print(stop_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenizer and process the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texts_clean = []\n",
    "texts_aux = []\n",
    "aux = []\n",
    "\n",
    "for article in texts_labels_np:\n",
    "        # Text to lower case\n",
    "        text = article[0].lower()\n",
    "        # Tokenize and Remove punctuation\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        # Remove stop words\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Stemming\n",
    "        for token in tokens:\n",
    "                aux.append(stemmer_cs.stem(token))\n",
    "        tokens = aux\n",
    "        \n",
    "        texts_aux.append(tokens)\n",
    "        texts_aux.append(article[1])\n",
    "        texts_clean.append(texts_aux)\n",
    "        texts_aux = []\n",
    "        aux=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming labels into numbers [business, entertainment, politics, sport, tech] -- [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts_clean:\n",
    "        if text[1]=='business':\n",
    "                text[1]=0\n",
    "        if text[1]=='entertainment':\n",
    "                text[1]=1\n",
    "        if text[1]=='politics':\n",
    "                text[1]=2\n",
    "        if text[1]=='sport':\n",
    "                text[1]=3\n",
    "        if text[1]=='tech':\n",
    "                text[1]=4\n",
    "\n",
    "text_clean_np = np.array(texts_clean)\n",
    "text_clean_pd = pd.DataFrame(texts_labels, columns=['text','label'])\n",
    "\n",
    "tokenized_texts = []\n",
    "labels = []\n",
    "for article in texts_clean:\n",
    "        tokenized_texts.append(article[0])\n",
    "        labels.append(article[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test set and prepare it for the doc2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "# Data file path\n",
    "dataPath = '../data/'\n",
    "\n",
    "# Train ratio\n",
    "train_ratio = 0.85\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(tokenized_texts, labels, test_size=1 - train_ratio, stratify=labels)\n",
    "#x_dev, x_test, t_dev, t_test = train_test_split(x_test, t_test, test_size=0.5, stratify=t_test)\n",
    "\n",
    "##\n",
    "##\n",
    "# Word2Vec\n",
    "##\n",
    "##\n",
    "\n",
    "from  gensim.models.doc2vec import TaggedDocument\n",
    "from  gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# We contruct the training and testing dataframe for the word2vec\n",
    "## Training\n",
    "words_train = pd.DataFrame(np.array(x_train), columns=['words'])\n",
    "tags_train = pd.DataFrame(np.array(t_train), columns=['tags'])\n",
    "documents_train = pd.concat([words_train, tags_train], axis=1)\n",
    "# Testing\n",
    "words_test = pd.DataFrame(np.array(x_test), columns=['words'])\n",
    "tags_test = pd.DataFrame(np.array(t_test), columns=['tags'])\n",
    "documents_test = pd.concat([words_test, tags_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the document vectors\n",
    "def tag_docs(docs):\n",
    "    tagged = docs.apply(lambda r: gensim.models.doc2vec.TaggedDocument(words=r[0], tags=[r[1]]), axis=1)\n",
    "    return tagged\n",
    "\n",
    "# Train the doc2vec model\n",
    "def train_doc2vec_model(tagged_docs, window, vector_size):\n",
    "    sents = tagged_docs.values\n",
    "    doc2vec_model = Doc2Vec(sents, vector_size=vector_size, window=window, epochs=20, dm=0)\n",
    "    return doc2vec_model\n",
    "\n",
    "# Construct the final vector feature for the classifier\n",
    "def vec_for_learning(doc2vec_model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    # Unzipping the values\n",
    "    targets, regressors = zip(*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged = tag_docs(documents_train)\n",
    "test_tagged = tag_docs(documents_test)\n",
    "\n",
    "\n",
    "word2vec_model = train_doc2vec_model(train_tagged, 15, 5)\n",
    "\n",
    "\n",
    "y_train, X_train = vec_for_learning(word2vec_model, train_tagged)\n",
    "y_test, X_test = vec_for_learning(word2vec_model, test_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy with LOGISTIC REGRESSION 0.978978978978979\n",
      "Testing F1 score with LOGISTIC REGRESSION: 0.9789514660534387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy with LOGISTIC REGRESSION %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score with LOGISTIC REGRESSION: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy with RANDOM FOREST 0.9579579579579579\n",
      "Testing F1 score with RANDOM FOREST: 0.9578063433777974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=150,n_jobs=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Testing accuracy with RANDOM FOREST %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score with RANDOM FOREST: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy with SVM 0.978978978978979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_model = LinearSVC(C=0.01)\n",
    "svm_model.fit(X_train, y_train)\n",
    "print (\"Testing accuracy with SVM %s\" \n",
    "       % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning unlabeled articles to users by their profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demostration of how the application would function we created a mock up data set of unlabeled articles with two articles for each category and set of users that are defined by the topics they are interested in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first user is interested in sports and politics, the second in tech and entertainment,the third in tech and sports and the fourth in business and politics.  Having theseprofiles set, we will test whether the trained model is able to deliver articles that correspond to user topic preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile1=['sports', 'politics']\n",
    "profile2=['tech', 'entertainment']\n",
    "profile3=['tech', 'sports']\n",
    "profile4=['business','politics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the text of the articles and label them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entertainment', 'business', 'sports', 'politics', 'tech']\n"
     ]
    }
   ],
   "source": [
    "labels_demo = []\n",
    "filepath = '../data_demo/'\n",
    "files = os.listdir(filepath)\n",
    "for name in files:\n",
    "    if os.path.isdir(filepath+name): \n",
    "        labels_demo.append(name)\n",
    "        \n",
    "print(labels_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment\n",
      "entertainment\n",
      "business\n",
      "business\n",
      "sports\n",
      "sports\n",
      "politics\n",
      "politics\n",
      "tech\n",
      "tech\n"
     ]
    }
   ],
   "source": [
    "text_demo = []   \n",
    "texts_aux_demo = [] \n",
    "texts_labels_demo = [] \n",
    "for label in labels_demo:\n",
    "    path = filepath+label+'/*.txt'\n",
    "    files = glob.glob(path)\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name, 'r',encoding='ISO-8859-1') as f:\n",
    "                texts_aux_demo.append(f.read())\n",
    "                texts_aux_demo.append(label)\n",
    "                print(label)\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise\n",
    "        texts_labels_demo.append(texts_aux_demo)\n",
    "        texts_aux_demo=[]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles of **business**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dollar gains on Greenspan speech\n",
      "\n",
      "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
      "\n",
      "And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
      "\n",
      "Worries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n",
      "Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
      "\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "\n",
      "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
      "\n",
      "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
      "\n",
      "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for article in texts_labels_demo:\n",
    "    if article[1]=='business':\n",
    "        print(article[0])\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles for **entertainment**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarre joins fairytale celebration\n",
      "\n",
      "French musician Jean-Michel Jarre is to perform at a concert in Copenhagen to mark the bicentennial of the birth of writer Hans Christian Andersen.\n",
      "\n",
      "Denmark is holding a three-day celebration of the life of the fairy-tale author, with a concert at Parken stadium on 2 April. Other stars are expected to join the line-up in the coming months, and the Danish royal family will attend. \"Christian Andersen's fairy tales are timeless and universal,\" said Jarre. \"For all of us, at any age there is always - beyond the pure enjoyment of the tale - a message to learn.\" There are year-long celebrations planned across the world to celebrate Andersen and his work, which includes The Emperor's New Clothes and The Little Mermaid. Denmark's Crown Prince Frederik and Crown Princess Mary visited New York on Monday to help promote the festivities. The pair were at a Manhattan library to honour US literary critic Harold Bloom \"the international icon we thought we knew so well\".\n",
      "\n",
      "\"Bloom recognizes the darker aspects of Andersen's authorship,\" Prince Frederik said. Bloom is to be formally presented with the Hans Christian Andersen Award this spring in Anderson's hometown of Odense. The royal couple also visited the Hans Christian Anderson School complex, where Queen Mary read The Ugly Duckling to the young audience. Later at a gala dinner, Danish supermodel Helena Christensen was named a Hans Christian Andersen ambassador. Other ambassadors include actors Harvey Keitel and Sir Roger Moore, athlete Cathy Freeman and Brazilian soccer legend Pele.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n",
      "Gallery unveils interactive tree\n",
      "\n",
      "A Christmas tree that can receive text messages has been unveiled at London's Tate Britain art gallery.\n",
      "\n",
      "The spruce has an antenna which can receive Bluetooth texts sent by visitors to the Tate. The messages will be \"unwrapped\" by sculptor Richard Wentworth, who is responsible for decorating the tree with broken plates and light bulbs. It is the 17th year that the gallery has invited an artist to dress their Christmas tree. Artists who have decorated the Tate tree in previous years include Tracey Emin in 2002.\n",
      "\n",
      "The plain green Norway spruce is displayed in the gallery's foyer. Its light bulb adornments are dimmed, ordinary domestic ones joined together with string. The plates decorating the branches will be auctioned off for the children's charity ArtWorks. Wentworth worked as an assistant to sculptor Henry Moore in the late 1960s. His reputation as a sculptor grew in the 1980s, while he has been one of the most influential teachers during the last two decades. Wentworth is also known for his photography of mundane, everyday subjects such as a cigarette packet jammed under the wonky leg of a table.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for article in texts_labels_demo:\n",
    "    \n",
    "    if article[1]=='entertainment':\n",
    "        print(article[0])\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles for **politics**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchdog probes e-mail deletions\n",
      "\n",
      "The information commissioner says he is urgently asking for details of Cabinet Office orders telling staff to delete e-mails more than three months old.\n",
      "\n",
      "Richard Thomas \"totally condemned\" the deletion of e-mails to prevent their disclosure under freedom of information laws coming into force on 1 January. Government guidance said e-mails should only be deleted if they served \"no current purpose\", Mr Thomas said. The Tories and the Lib Dems have questioned the timing of the new rules.\n",
      "\n",
      "Tory leader Michael Howard has written to Tony Blair demanding an explanation of the new rules on e-mail retention. On Monday Lib Dem constitutional affairs committee chairman Alan Beith warned that the deletion of millions of government e-mails could harm the ability of key probes like the Hutton Inquiry. The timing of the new rules just before the Freedom of Information Act comes into forces was \"too unlikely to have been a coincidence\", Mr Beith said. But a Cabinet Office spokeswoman said the move was not about the new laws or \"the destruction of important records\". Mr Beith urged the information commissioner to look at how the \"e-mail regime\" could \"support the freedom of information regime\".\n",
      "\n",
      "Mr Thomas said: \"The new Act of Parliament makes it very clear that to destroy records in order to prevent their disclosure becomes a criminal offence.\" He said there was already clear guidance on the retention of e-mails contained in a code of practice from the lord chancellor. All e-mails are subject to the freedom of information laws, but the important thing was the content of the e-mail, said Mr Thomas.\n",
      "\n",
      "\"If in doubt retain, that has been the long-standing principle of the civil service and public authorities. It's only when you've got no further use for the particular record that it may be legitimate to destroy it. \"But any deliberate destruction to avoid the possibility of later disclosure is to be totally condemned.\" The Freedom of Information Act will cover England, Wales and Northern Ireland from next year. Similar measures are being brought in at the same time in Scotland. It provides the public with a right of access to information held by about 100,000 public bodies, subject to various exemptions. Its implementation will be monitored by the information commissioner.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n",
      "Labour plans maternity pay rise\n",
      "\n",
      "Maternity pay for new mothers is to rise by Â£1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt.\n",
      "\n",
      "It would mean paid leave would be increased to nine months by 2007, Ms Hewitt told GMTV's Sunday programme. Other plans include letting maternity pay be given to fathers and extending rights to parents of older children. The Tories dismissed the maternity pay plan as \"desperate\", while the Liberal Democrats said it was misdirected.\n",
      "\n",
      "Ms Hewitt said: \"We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks. \"We are going to extend the pay to nine months by 2007 and the aim is to get it right up to the full 12 months by the end of the next Parliament.\" She said new mothers were already entitled to 12 months leave, but that many women could not take it as only six of those months were paid. \"We have made a firm commitment. We will definitely extend the maternity pay, from the six months where it now is to nine months, that's the extra Â£1,400.\" She said ministers would consult on other proposals that could see fathers being allowed to take some of their partner's maternity pay or leave period, or extending the rights of flexible working to carers or parents of older children. The Shadow Secretary of State for the Family, Theresa May, said: \"These plans were announced by Gordon Brown in his pre-budget review in December and Tony Blair is now recycling it in his desperate bid to win back women voters.\"\n",
      "\n",
      "She said the Conservatives would announce their proposals closer to the General Election. Liberal Democrat spokeswoman for women Sandra Gidley said: \"While mothers would welcome any extra maternity pay the Liberal Democrats feel this money is being misdirected.\" She said her party would boost maternity pay in the first six months to allow more women to stay at home in that time.\n",
      "\n",
      "Ms Hewitt also stressed the plans would be paid for by taxpayers, not employers. But David Frost, director general of the British Chambers of Commerce, warned that many small firms could be \"crippled\" by the move. \"While the majority of any salary costs may be covered by the government's statutory pay, recruitment costs, advertising costs, retraining costs and the strain on the company will not be,\" he said. Further details of the government's plans will be outlined on Monday. New mothers are currently entitled to 90% of average earnings for the first six weeks after giving birth, followed by Â£102.80 a week until the baby is six months old.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for article in texts_labels_demo:\n",
    "    \n",
    "    if article[1]=='politics':\n",
    "        print(article[0])\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles for **sports**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O'Sullivan could run in Worlds\n",
      "\n",
      "Sonia O'Sullivan has indicated that she would like to participate in next month's World Cross Country Championships in St Etienne.\n",
      "\n",
      "Athletics Ireland have hinted that the 35-year-old Cobh runner may be included in the official line-up for the event in France on 19-20 March. Provincial teams were selected after last Saturday's Nationals in Santry and will be officially announced this week. O'Sullivan is at present preparing for the London marathon on 17 April. The participation of O'Sullivan, currentily training at her base in Australia, would boost the Ireland team who won the bronze three years agio. The first three at Santry last Saturday, Jolene Byrne, Maria McCambridge and Fionnualla Britton, are automatic selections and will most likely form part of the long-course team. O'Sullivan will also take part in the Bupa Great Ireland Run on 9 April in Dublin.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n",
      "Claxton hunting first major medal\n",
      "\n",
      "British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid.\n",
      "\n",
      "The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title. \"I am quite confident,\" said Claxton. \"But I take each race as it comes. \"As long as I keep up my training but not do too much I think there is a chance of a medal.\" Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week's Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.\n",
      "\n",
      "For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form. In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions. Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for article in texts_labels_demo:\n",
    "    \n",
    "    if article[1]=='sports':\n",
    "        print(article[0])\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles for **tech**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China net cafe culture crackdown\n",
      "\n",
      "Chinese authorities closed 12,575 net cafes in the closing months of 2004, the country's government said.\n",
      "\n",
      "According to the official news agency most of the net cafes were closed down because they were operating illegally. Chinese net cafes operate under a set of strict guidelines and many of those most recently closed broke rules that limit how close they can be to schools. The move is the latest in a series of steps the Chinese government has taken to crack down on what it considers to be immoral net use.\n",
      "\n",
      "The official Xinhua News Agency said the crackdown was carried out to create a \"safer environment for young people in China\". Rules introduced in 2002 demand that net cafes be at least 200 metres away from middle and elementary schools. The hours that children can use net cafes are also tightly regulated. China has long been worried that net cafes are an unhealthy influence on young people. The 12,575 cafes were shut in the three months from October to December. China also tries to dictate the types of computer games people can play to limit the amount of violence people are exposed to.\n",
      "\n",
      "Net cafes are hugely popular in China because the relatively high cost of computer hardware means that few people have PCs in their homes. This is not the first time that the Chinese government has moved against net cafes that are not operating within its strict guidelines. All the 100,000 or so net cafes in the country are required to use software that controls what websites users can see. Logs of sites people visit are also kept. Laws on net cafe opening hours and who can use them were introduced in 2002 following a fire at one cafe that killed 25 people. During the crackdown following the blaze authorities moved to clean up net cafes and demanded that all of them get permits to operate. In August 2004 Chinese authorities shut down 700 websites and arrested 224 people in a crackdown on net porn. At the same time it introduced new controls to block overseas sex sites. The Reporters Without Borders group said in a report that Chinese government technologies for e-mail interception and net censorship are among the most highly developed in the world.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n",
      "Ink helps drive democracy in Asia\n",
      "\n",
      "The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country's elections as part of a drive to prevent multiple voting.\n",
      "\n",
      "This new technology is causing both worries and guarded optimism among different sectors of the population. In an effort to live up to its reputation in the 1990s as \"an island of democracy\", the Kyrgyz President, Askar Akaev, pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections. The US government agreed to fund all expenses associated with this decision.\n",
      "\n",
      "The Kyrgyz Republic is seen by many experts as backsliding from the high point it reached in the mid-1990s with a hastily pushed through referendum in 2003, reducing the legislative branch to one chamber with 75 deputies. The use of ink is only one part of a general effort to show commitment towards more open elections - the German Embassy, the Soros Foundation and the Kyrgyz government have all contributed to purchase transparent ballot boxes.\n",
      "\n",
      "The actual technology behind the ink is not that complicated. The ink is sprayed on a person's left thumb. It dries and is not visible under normal light.\n",
      "\n",
      "However, the presence of ultraviolet light (of the kind used to verify money) causes the ink to glow with a neon yellow light. At the entrance to each polling station, one election official will scan voter's fingers with UV lamp before allowing them to enter, and every voter will have his/her left thumb sprayed with ink before receiving the ballot. If the ink shows under the UV light the voter will not be allowed to enter the polling station. Likewise, any voter who refuses to be inked will not receive the ballot. These elections are assuming even greater significance because of two large factors - the upcoming parliamentary elections are a prelude to a potentially regime changing presidential election in the Autumn as well as the echo of recent elections in other former Soviet Republics, notably Ukraine and Georgia. The use of ink has been controversial - especially among groups perceived to be pro-government.\n",
      "\n",
      "Widely circulated articles compared the use of ink to the rural practice of marking sheep - a still common metaphor in this primarily agricultural society.\n",
      "\n",
      "The author of one such article began a petition drive against the use of the ink. The greatest part of the opposition to ink has often been sheer ignorance. Local newspapers have carried stories that the ink is harmful, radioactive or even that the ultraviolet readers may cause health problems. Others, such as the aggressively middle of the road, Coalition of Non-governmental Organizations, have lauded the move as an important step forward. This type of ink has been used in many elections in the world, in countries as varied as Serbia, South Africa, Indonesia and Turkey. The other common type of ink in elections is indelible visible ink - but as the elections in Afghanistan showed, improper use of this type of ink can cause additional problems. The use of \"invisible\" ink is not without its own problems. In most elections, numerous rumors have spread about it.\n",
      "\n",
      "In Serbia, for example, both Christian and Islamic leaders assured their populations that its use was not contrary to religion. Other rumours are associated with how to remove the ink - various soft drinks, solvents and cleaning products are put forward. However, in reality, the ink is very effective at getting under the cuticle of the thumb and difficult to wash off. The ink stays on the finger for at least 72 hours and for up to a week. The use of ink and readers by itself is not a panacea for election ills. The passage of the inking law is, nevertheless, a clear step forward towards free and fair elections.\" The country's widely watched parliamentary elections are scheduled for 27 February.\n",
      "\n",
      "David Mikosz works for the IFES, an international, non-profit organisation that supports the building of democratic societies.\n",
      "\n",
      "########\n",
      "########\n",
      "########\n",
      "########\n"
     ]
    }
   ],
   "source": [
    "for article in texts_labels_demo:\n",
    "    \n",
    "    if article[1]=='tech':\n",
    "        print(article[0])\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')\n",
    "        print('########')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to clean the data and process it for the word2vec embedding algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['jarr', 'joi', 'fairytal', 'celebratio', 'french', 'musicia', 'jea', 'michel', 'jarr', 'perform', 'conc', 'copenhag', 'mark', 'bicentennial', 'birth', 'wri', 'han', 'christia', 'ander', 'denmark', 'holding', 'three', 'day', 'celebratio', 'lif', 'fairy', 'tal', 'author', 'conc', 'park', 'stadium', '2', 'april', 'star', 'expected', 'joi', 'lin', 'coming', 'month', 'danish', 'royal', 'family', 'att', 'christia', 'ander', 'fairy', 'tal', 'timeless', 'universal', 'said', 'jarr', 'us', 'age', 'alway', 'beyo', 'pur', 'enjoym', 'tal', 'messag', 'lear', 'year', 'long', 'celebratio', 'planned', 'across', 'world', 'celebra', 'ander', 'work', 'includ', 'emperor', 'new', 'cloth', 'littl', 'mermaid', 'denmark', 'crow', 'princ', 'frederik', 'crow', 'princess', 'mary', 'visited', 'new', 'york', 'monday', 'help', 'promo', 'festivitie', 'pair', 'manhatta', 'library', 'honour', 'us', 'literary', 'critic', 'harold', 'bloom', 'international', 'ico', 'though', 'knew', 'well', 'bloom', 'recogniz', 'dark', 'aspec', 'ander', 'authorship', 'princ', 'frederik', 'said', 'bloom', 'formally', 'presented', 'han', 'christia', 'ander', 'award', 'spring', 'anderso', 'hometow', 'ode', 'royal', 'coupl', 'also', 'visited', 'han', 'christia', 'anderso', 'school', 'complex', 'quee', 'mary', 'read', 'ugly', 'duckling', 'young', 'audienc', 'later', 'gala', 'dinn', 'danish', 'supermodel', 'helena', 'chri', 'named', 'han', 'christia', 'ander', 'ambassador', 'ambassador', 'includ', 'actor', 'harvey', 'keitel', 'sir', 'roger', 'moor', 'athl', 'cathy', 'freema', 'brazilia', 'socc', 'leg', 'pel'], 'entertainment']\n"
     ]
    }
   ],
   "source": [
    "texts_labels_demo_np = np.array(texts_labels_demo)\n",
    "texts_clean_demo = []\n",
    "texts_aux_demo = []\n",
    "aux_demo = []\n",
    "for article in texts_labels_demo_np:\n",
    "        # Text to lower case\n",
    "        text_demo = article[0].lower()\n",
    "        # Tokenize and Remove punctuation\n",
    "        tokens = tokenizer.tokenize(text_demo)\n",
    "        # Remove stop words\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Stemming\n",
    "        for token in tokens:\n",
    "                aux_demo.append(stemmer_cs.stem(token))\n",
    "        tokens = aux_demo\n",
    "        \n",
    "        texts_aux_demo.append(tokens)\n",
    "        texts_aux_demo.append(article[1])\n",
    "        texts_clean_demo.append(texts_aux_demo)\n",
    "        texts_aux_demo = []\n",
    "        aux_demo=[]\n",
    "print(texts_clean_demo[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming labels into numbers [business, entertainment, politics, sport, tech] -- [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "10\n",
      "[1, 1, 0, 0, 3, 3, 2, 2, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "for text in texts_clean_demo:\n",
    "        if text[1]=='business':\n",
    "                text[1]=0\n",
    "        if text[1]=='entertainment':\n",
    "                text[1]=1\n",
    "        if text[1]=='politics':\n",
    "                text[1]=2\n",
    "        if text[1]=='sports':\n",
    "                text[1]=3\n",
    "        if text[1]=='tech':\n",
    "                text[1]=4\n",
    "text_clean_demo_np = np.array(texts_clean_demo)\n",
    "text_clean_demo_pd = pd.DataFrame(texts_labels_demo, columns=['text','label'])\n",
    "\n",
    "print(text_clean_demo_np.shape)\n",
    "\n",
    "tokenized_texts_demo = []\n",
    "labels_demo = []\n",
    "for article in texts_clean_demo:\n",
    "        tokenized_texts_demo.append(article[0])\n",
    "        labels_demo.append(article[1])\n",
    "print(len(tokenized_texts_demo))\n",
    "print(labels_demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               words  tags\n",
      "0  [jarr, joi, fairytal, celebratio, french, musi...     1\n",
      "1  [gallery, unveil, interactiv, tree, christma, ...     1\n",
      "2  [dollar, gai, greenspa, speech, dollar, hit, h...     0\n",
      "3  [ad, sal, boo, tim, war, profi, quarterly, pro...     0\n",
      "4  [sulliva, could, run, world, sonia, sulliva, i...     3\n",
      "5  [claxto, hunting, fir, major, medal, british, ...     3\n",
      "6  [watchdog, prob, e, mail, deletio, informatio,...     2\n",
      "7  [labour, pla, maternity, pay, ris, maternity, ...     2\n",
      "8  [china, net, caf, cultur, crackdow, chi, autho...     4\n",
      "9  [ink, help, driv, democracy, asia, kyrgyz, rep...     4\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct dataframe\n",
    "words_test_demo = pd.DataFrame(np.array(tokenized_texts_demo), columns=['words'])\n",
    "tags_test_demo = pd.DataFrame(np.array(labels_demo), columns=['tags'])\n",
    "documents_test_demo = pd.concat([words_test_demo, tags_test_demo], axis=1)\n",
    "print(documents_test_demo)\n",
    "test_tagged_demo = tag_docs(documents_test_demo)\n",
    "y_test_demo, X_test_demo = vec_for_learning(word2vec_model, test_tagged_demo)\n",
    "print(len(y_test_demo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 3, 3, 2, 2, 4, 4]\n",
      "[1 1 0 0 3 3 2 2 4 2]\n",
      "Testing accuracy with LOGISTIC REGRESSION 0.9\n",
      "Testing F1 score with LOGISTIC REGRESSION: 0.8933333333333333\n",
      "[1, 1, 0, 0, 3, 3, 2, 2, 4, 4]\n",
      "[1 1 0 0 3 3 2 2 4 2]\n",
      "Testing accuracy with RANDOM FOREST 0.9\n",
      "Testing F1 score with RANDOM FOREST: 0.8933333333333333\n",
      "[1, 1, 0, 0, 3, 3, 2, 2, 4, 4]\n",
      "[1 1 0 0 3 3 2 2 4 2]\n",
      "Testing accuracy with SVM 0.9\n",
      "Testing F1 score with SVM: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model\n",
    "y_pred_demo = logreg.predict(X_test_demo)\n",
    "print(labels_demo)\n",
    "print(y_pred_demo)\n",
    "print('Testing accuracy with LOGISTIC REGRESSION %s' % accuracy_score(y_test_demo, y_pred_demo))\n",
    "print('Testing F1 score with LOGISTIC REGRESSION: {}'.format(f1_score(y_test_demo, y_pred_demo, average='weighted')))\n",
    "#Random forest model\n",
    "y_pred_demo = rf_model.predict(X_test_demo)\n",
    "print(labels_demo)\n",
    "print(y_pred_demo)\n",
    "print('Testing accuracy with RANDOM FOREST %s' % accuracy_score(y_test_demo, y_pred_demo))\n",
    "print('Testing F1 score with RANDOM FOREST: {}'.format(f1_score(y_test_demo, y_pred_demo, average='weighted')))\n",
    "# SVM model\n",
    "y_pred_demo = svm_model.predict(X_test_demo)\n",
    "print(labels_demo)\n",
    "print(y_pred_demo)\n",
    "print('Testing accuracy with SVM %s' % accuracy_score(y_test_demo, y_pred_demo))\n",
    "print('Testing F1 score with SVM: {}'.format(f1_score(y_test_demo, y_pred_demo, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would include delivering the classified articles to the respective users. We can see how all the models performce is the same, and all of the them have correctly classiffied 9 out 10 articles from out mock up database. So each user would received the following articles:\n",
    "   - profile1 --> will received  2 sport articles and 3 politics articles.\n",
    "   - profile2 --> will received 1 tech article and 2 entertainment articles.\n",
    "   - profile3 --> will received 1 tech article and 2 entertainment sports.\n",
    "   - profile4 --> will received  2 business articles and 3 politics articles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
