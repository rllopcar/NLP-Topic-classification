{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vector Implementation for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import errno\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "filepath = '../data/bbc/'\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the labels names and saving then in \"labels_names\"\n",
    "files = os.listdir(filepath)\n",
    "for name in files:\n",
    "    if os.path.isdir(filepath+name): \n",
    "        labels.append(name)\n",
    "        \n",
    "text = []   \n",
    "texts_aux = [] \n",
    "texts_labels = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the content of each .txt document and label the content of each article.\n",
    "for label in labels:\n",
    "    path = filepath+label+'/*.txt'\n",
    "    files = glob.glob(path)\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name, 'r',encoding='ISO-8859-1') as f:\n",
    "                texts_aux.append(f.read())\n",
    "                texts_aux.append(label)\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise\n",
    "        texts_labels.append(texts_aux)\n",
    "        texts_aux=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled texts stored in numpy array\n",
    "texts_labels_np = np.array(texts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeled texts stored in pandas dataframe\n",
    "df = pd.DataFrame(texts_labels, columns=['text','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: Tokenize each text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "## Load library for removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "##nltk.download('stopwords') --> First time has to be downloaded\n",
    "\n",
    "# Import libraries for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "stemmer_ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem.cistem import Cistem\n",
    "stemmer_cs = Cistem()\n",
    "\n",
    "# Import lemmatization libraries\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "#nltk.download('wordnet')--> First time has to be downloaded \n",
    "\n",
    "# Load stop words \n",
    "stop_words = stopwords.words('english')\n",
    "#print(stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer and process the raw text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "texts_clean = []\n",
    "texts_aux = []\n",
    "aux = []\n",
    "\n",
    "for article in texts_labels_np:\n",
    "        # Text to lower case\n",
    "        text = article[0].lower()\n",
    "        # Tokenize and Remove punctuation\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        # Remove stop words\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Stemming\n",
    "        for token in tokens:\n",
    "                aux.append(stemmer_cs.stem(token))\n",
    "        tokens = aux\n",
    "        \n",
    "        texts_aux.append(tokens)\n",
    "        texts_aux.append(article[1])\n",
    "        texts_clean.append(texts_aux)\n",
    "        texts_aux = []\n",
    "        aux=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming labels into numbers [business, entertainment, politics, sport, tech] -- [0,1,2,3,4]\n",
    "for text in texts_clean:\n",
    "        if text[1]=='business':\n",
    "                text[1]=0\n",
    "        if text[1]=='entertainment':\n",
    "                text[1]=1\n",
    "        if text[1]=='politics':\n",
    "                text[1]=2\n",
    "        if text[1]=='sport':\n",
    "                text[1]=3\n",
    "        if text[1]=='tech':\n",
    "                text[1]=4\n",
    "\n",
    "text_clean_np = np.array(texts_clean)\n",
    "text_clean_pd = pd.DataFrame(texts_labels, columns=['text','label'])\n",
    "\n",
    "tokenized_texts = []\n",
    "labels = []\n",
    "for article in texts_clean:\n",
    "        tokenized_texts.append(article[0])\n",
    "        labels.append(article[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.load('../data/features/tokenized.npy')\n",
    "x = np.array(tokenized_texts)\n",
    "#y = np.load('../data/features/labels.npy')\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of features extracted: 14850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "countvect = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None, \n",
    "    min_df=2,\n",
    "    max_df=0.5,\n",
    "    ngram_range=(1,1))\n",
    "\n",
    "X = countvect.fit_transform(x)\n",
    "Y = y\n",
    "\n",
    "print (\"no of features extracted:\", X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation and test set and prepare them for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (1089, 14850)\n",
      "validation size: (468, 14850)\n",
      "test size: (668, 14850)\n",
      "class distribution in training set: 3    250\n",
      "0    250\n",
      "2    204\n",
      "4    196\n",
      "1    189\n",
      "dtype: int64\n",
      "class distribution in validation set: 3    107\n",
      "0    107\n",
      "2     88\n",
      "4     85\n",
      "1     81\n",
      "dtype: int64\n",
      "class distribution in test set: 3    154\n",
      "0    153\n",
      "2    125\n",
      "4    120\n",
      "1    116\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)\n",
    "\n",
    "print (\"train size:\", X_train.shape)\n",
    "print (\"validation size:\", X_val.shape)\n",
    "print (\"test size:\", X_test.shape)\n",
    "print (\"class distribution in training set:\", pd.Series(y_train).value_counts())\n",
    "print (\"class distribution in validation set:\", pd.Series(y_val).value_counts())\n",
    "print (\"class distribution in test set:\", pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.9700854700854701\n",
      "Accuracy for C=0.05: 0.967948717948718\n",
      "Accuracy for C=0.25: 0.967948717948718\n",
      "Accuracy for C=0.5: 0.967948717948718\n",
      "Accuracy for C=1: 0.967948717948718\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9730538922155688\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[147   0   4   1   1]\n",
      " [  0 114   2   0   0]\n",
      " [  5   1 118   0   1]\n",
      " [  1   0   0 153   0]\n",
      " [  1   0   0   1 118]]\n"
     ]
    }
   ],
   "source": [
    "c_mat = confusion_matrix(y_test,model.predict(X_test))\n",
    "print (\"Confusion Matrix:\\n\", c_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.001: 0.9658119658119658\n",
      "Accuracy for C=0.005: 0.9658119658119658\n",
      "Accuracy for C=0.01: 0.9658119658119658\n",
      "Accuracy for C=0.05: 0.9615384615384616\n",
      "Accuracy for C=0.1: 0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "for c in [0.001, 0.005, 0.01, 0.05, 0.1]:    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.9775449101796407\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[151   0   2   0   0]\n",
      " [  0 114   2   0   0]\n",
      " [  5   1 118   0   1]\n",
      " [  1   0   0 153   0]\n",
      " [  2   0   0   1 117]]\n"
     ]
    }
   ],
   "source": [
    "c_mat = confusion_matrix(y_test,model.predict(X_test))\n",
    "print (\"Confusion Matrix:\\n\", c_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=150, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=300, max_depth=150,n_jobs=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[150   0   3   0   0]\n",
      " [  3 112   1   0   0]\n",
      " [  9   0 115   0   1]\n",
      " [  0   0   0 154   0]\n",
      " [  6   3   1   0 110]]\n",
      "\n",
      "Accuracy:  0.9595808383233533\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "c_mat = confusion_matrix(y_test,y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print (\"Confusion Matrix:\\n\", c_mat)\n",
    "print (\"\\nAccuracy: \",acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
